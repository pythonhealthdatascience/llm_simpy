{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73bdb0ff-3c46-4975-bc88-37fbf72a6e6a",
   "metadata": {},
   "source": [
    "# Stroke+Rehab models\n",
    "\n",
    "> This is a work in progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8b392e-9a99-4333-a6ca-fe6cd173863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ab641d-4d8d-4fa6-8123-2ef096b8bc9f",
   "metadata": {},
   "source": [
    "###  Model outputs\n",
    "\n",
    "The results of the two generated simulations models were identical to 2 decimal places. The results for stage 1 and stage 2 models are reported and compared graphically below in {numref}`asu_comparison_fig` and {numref}`rehab_comparison_fig`. The figures show that the **probability of delay** and **ward occupancy** match across the acute and rehabilitation wards within the 2 models.\n",
    "\n",
    "The outputs from the generated models results replicated the results reported in the original article {cite:p}`Monks2016`; although we note that did not run all of the experiments reported in the article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a295cf-2e8f-42f8-bff0-6bb7f8f39a40",
   "metadata": {},
   "source": [
    "```{figure} ../../03_stroke/asu_comparison.png\n",
    "---\n",
    "height: 500px\n",
    "name: asu_comparison_fig\n",
    "---\n",
    "Acute stroke unit outputs: comparison of stage 1 and stage 2 models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27340149-79dc-4e42-891c-c45444e4456b",
   "metadata": {},
   "source": [
    "```{figure} ../../03_stroke/rehab_comparison.png\n",
    "---\n",
    "height: 500px\n",
    "name: rehab_comparison_fig\n",
    "---\n",
    "Rehabilitation unit outputs: comparison of stage 1 and stage 2 models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4f6996-2577-47c3-9de3-69460f7d1907",
   "metadata": {},
   "source": [
    "### Model code\n",
    "\n",
    "The final code files from stage 1 and stage 2 (our internal replication) for the stroke capacity planning model have some substantial differences.  **To describe**\n",
    "\n",
    "Disregarding comments and documentation, stage 1 generated a simpy model consisting of 436 line of code and stage 2 generated 531 lines of code.  Both models passed the same batch of 34 verification tests.\n",
    "\n",
    "**To-do**: discuss the LLMs design of `Experiment` for stage 2 versus stage 1. Quite different and less fun to setup!  Show code snippets.\n",
    "\n",
    "**To-do**: discuss interface code - this is currently not included in code totals.\n",
    "\n",
    "**To-do** Add table describing differences in classes and functions.\n",
    "<!-- ```{list-table} Description of model code components Stage 1 versus Stage 2. (stage 2 inside of brackets)\n",
    ":header-rows: 1\n",
    ":name: ccu_component_comparison\n",
    "\n",
    "* - Component\n",
    "  - Attributes (stage 2)\n",
    "  - Methods/Functions (stage 2)\n",
    "* - **Experiment class**\n",
    "  - 13 (27)\n",
    "  - 3 (2)\n",
    "* - **CCU model logic class**\n",
    "  - 4 (9)\n",
    "  - 10 (12)\n",
    "* - **Functions**\n",
    "  - N/A\n",
    "  - 6 (6)\n",
    "``` -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bf96b7-66e3-4a40-89d9-ae1a9eae0a68",
   "metadata": {},
   "source": [
    "#### Complexity of setup and use of models\n",
    "\n",
    "Although the stage 1 and 2 models produced identical outputs the internal implementation of the `Experiment` class varied substantially. The code generated by the LLM led to quite different interfaces to setup and create an instance of `Experiment` and then to access internal parameters.\n",
    "\n",
    "For example, in the stage 1 model the code to setup an experiment that simulated a 5% increase in stroke patients, and then check the parameter value was as follows:\n",
    "\n",
    "```python\n",
    "# setup experiment\n",
    "default_experiment = Experiment(stroke_mean=1.2*1.05)\n",
    "\n",
    "# access and check parameter value\n",
    "print(default_experiment.stroke_mean)\n",
    "```\n",
    "\n",
    "The equivalent code in stage 2 involved an additional line of code to create a experimentation dictionary and a *collection data-structure* approach to access the internal parameters.\n",
    "\n",
    "```python\n",
    "# setup paramater dictionary\n",
    "experiment_params = {\"patient_types\": {\"Stroke\": {\"interarrival_time\": 1.2 * 1.05}}}\n",
    "\n",
    "# pass to Experiment. LLM provided code that updates internal parameter dictionaries\n",
    "future_demand_experiment = Experiment(experiment_params)\n",
    "\n",
    "# access and check parameter value\n",
    "print(future_demand_experiment.params['patient_types']['Stroke']['interarrival_time'])\n",
    "```\n",
    "\n",
    "We do not argue that either of the approaches generated by the LLM is optimal. Rather that there are pro's and con's to their implementations. Stage 1 code offers a simple interface, but does not choose a clear naming convention (`stroke_mean` is not specific to inter-arrival time). Stage 1 also does not clearly separate model parameters from the outputs of the experiment. Stage 2 code requires more code and requires a user to understand Python dictionaries. Stage 2's hierarchy to access parameters is more complex than stage 1's (including the internal workings of Experiment), but it uses clear specific naming conventions for patients types and their different parameters configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55904128-5b1f-479b-8c6f-c373f27dd0ad",
   "metadata": {},
   "source": [
    "#### Lines of code data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897e152-9282-4a13-be67-e900ce2183b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygount --suffix=py --format=summary ../../03_stroke/stroke_rehab_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ac215-9bec-4961-b739-bc65c7f025d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for stroke\n",
    "# !pygount --suffix=py --format=summary ../../03_stroke/stroke_rehab_model.py\n",
    "# !pygount --suffix=py --format=summary ../../03_stroke/stroke_rehab_interface.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff78acd-aed3-464a-bb5f-9a098618e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygount --suffix=py --format=summary ../../03_stroke/s2_stroke_rehab_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0e4c4f-18e8-43e9-8e72-97bb85a80cce",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "\n",
    "In total 31 iterations of the model were used to build the model and interface. In stage 1 this consisted of 41 prompts passed to the LLM. The number of prompts increased to 57 in stage 2. In total **n** (**TO-DO**) additional prompts were needed in stage 2 to fix a variable type bug introduced by the LLM for representing \"patient type\" across the acute and rehab sections of the model. Stage 2 required 4 additional prompts for introducing common random numbers streams to the LLM struggling to assign streams across model activities  (**TO-do**: check with Alison).\n",
    "\n",
    "The table below provides a summary of the differences at each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e0a81-6ef8-4f32-81cc-0cd35ac78fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_last_row(df: pd.DataFrame) -> list[str]:\n",
    "    '''\n",
    "    highlight the last row (for Totals) in a DataFrame in BOLD.\n",
    "    \n",
    "    Source:\n",
    "    -------\n",
    "\n",
    "    Adapted from stackoverflow\n",
    "    https://stackoverflow.com/questions/51938245/display-dataframe\n",
    "    -values-in-bold-font-in-one-row-only#59493062\n",
    "    '''\n",
    "    return ['font-weight: bold' if v == df.iloc[-1] else '' for v in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d9a90-e5fe-41fa-972b-3acd4abe3747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in prompt results table\n",
    "prompt_results = (\n",
    "    pd.read_csv(\"data/stroke_prompt_table.csv\",\n",
    "                index_col=['Iteration'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db7385-7076-4b1c-8b68-be09705ba5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_results.loc[len(prompt_results) + 1] = [\"Totals\"] + prompt_results.sum().tolist()[-3:]\n",
    "prompt_results.style.apply(highlight_last_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80894b1-be95-4b91-8024-d0ee211814f8",
   "metadata": {},
   "source": [
    "## LateX for manuscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdbe4d9-0428-4a1f-982b-cfedd4d6221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"The number of prompts given to the LLM \" \\\n",
    "          + \"at each iteration of the stroke capacity planning model).\"\n",
    "                \n",
    "print(prompt_results.style.to_latex(caption=caption))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87eb396-e04f-42e8-af99-a6915ac55855",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":style: plain\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
