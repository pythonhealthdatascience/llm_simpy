## Secondary aims

Our secondary aim is to investigate *prompt engineering*: the process of structuring text that can be interpreted and understood by a LLM in order to efficiently and effectively generate a DES model.

1. Investigate the balance of health service and problem information, general simulation information (such as input distributions, including warm-up periods, adding initial conditions, conducting a replication analysis, typical performance measures), and python, and simpy specific requirements that must be given to an LLM in order to generate a functional model.
2. From (1), investigate if there are better or worse ways of (re)phrasing the different inputs to an LLM in order to get a functionally correct model.
3.  Explore the variability of generated outputs by the same LLM and across two LLMs (e.g. Chat GPT3.5 versus Google Bard or ChatGPT3.5 versus ChatGPT4.0).


5. Compare LLMs for generating model code documentation and explanation that could be incorporated into a STARS enhanced output.